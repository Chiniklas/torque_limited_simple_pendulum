# Soft Actor Critic Control #

Type: Closed loop, learning based, model free

State/action space constraints: None

Optimal: Yes

Versatility: Swing-up and stabilization

## Theory # 

The soft actor critic (SAC) algorithm is a reinforcement learning (RL) 
method. It belongs to the class of so called 'model free' 
methods, i.e. no knowledge about the system to be controlled is 
assumed. Instead, the controller is trained via interaction with 
the system, such that a (sub-)optimal mapping from state space 
to control command is learned. The learning process is guided by 
a reward function that encodes the task, similar to the usage of 
cost functions in optimal control. 

SAC has two defining features. 
Firstly, the mapping from state space to control command is probabilistic. 
Secondly, the entropy of the contol output is maximized along with the reward 
function during training.
In theory, this leads to robust controllers and reduces the probability of 
ending up in suboptimal local minima.

For more information on SAC please refer to the original paper:

Haarnoja et al. 2018, Soft Actor-Critic: Off-Policy Maximum 
Entropy Deep Reinforcement Learning with a Stochastic Actor, 
url: https://arxiv.org/abs/1801.01290

## Requirements # 
- Stable Baselines (https://stable-baselines.readthedocs.io/en/master/guide/install.html)
- Tensorflow >=1.80, <=1.15
- Numpy
- PyYaml

## API # 
### Training #
Training of a controller can be started via:

    python train_sac.py

training parameters are located in: 
    
    /training/parameters.yaml

the parameters that can be set in this file are:

    torque_limit: float             # torque limit
    max_steps: int                  # maximum number of steps per training episode
    reward_type: str                # 'soft_binary_with_repellor' is the only one implemented by default
    dt: float                       # time step of simulator
    integrator: str                 # 'runge_kutta' or 'euler
    warm_start: bool                # whether to warm start the training from a previously trained model
    warm_start_path: str or path    # if warm starting, the path of the saved model relative to the project's root directory
    learning_rate: float            # learning rate of the RL algorithm

the resulting best model and log files will be saved in: 
    
    /training/best_model
    /training/tb_logs

:warning: **Attention**: when training is started, 
these directories will be deleted. So if you want to keep a trained 
model, move the saved files somewhere else.

The training progress can be observed with tensorboard. When starting 
from the root directory of the project:

    $> cd software/python/controllers/soft_actor_critic/training/
    $> tensorboard --logdir tb_logs

The default reward function used during training is
```math
\begin{equation}
r =  \exp{-(\theta - \pi)^2/(2*0.25^2)} - \exp{-(\theta - 0)^2/(2*0.25^2)}
\end{equation}
```
This encourages moving away from the stable fixed point of the system 
at $`\theta = 0`$ and spending most time at the target, the unstable 
fixed point $`\theta = \pi`$. Different reward functions can be used 
by modifying the *swingup_reward* method of the training environment with 
an appropriate *if* clause, and then selecting this reward function in 
the training parameters under the key *'reward_type'*. The training 
enviroment is located in *gym_environment.py*

### Control # 
The class for the SAC Controller is located in 
sac_controller.py

it is initialized by creating an instance as:

    controller = SacController(self, model_path=model_path, params_path=params_path)
        inputs:
            model_path: str or path, default: '/../../../data/models/sac_model.zip'
            params_path: str or path, default: '/../../../data/models/sac_parameters.zip'

The control output $`\mathbf{u}(\mathbf{x})`$ when given observed state $`\mathbf{x}`$ 
is generated by:

    controller.get_control_output(meas_pos, mean_vel, meas_tau, meas_time)

## Usage #
Once a model is trained, or using the provided pretrained model, the model 
can be tested in simulation with the sim_sac.py script:

    python sim_sac.py

if no arguments are given, the default pretrained model will be loaded, which is 
located in 

    /data/models/

starting with arguments for *model_path* and *params_path* will load 
data from these paths. For example, once a model was trained:

    python sim_sac.py --model_path training/best_model/best_model.zip --params_path training/parameters.yaml

The paths are expected to be in relation to *software/python/controllers/soft_actor_critic*

## Comments # 
Todo: comments on training convergence stability





